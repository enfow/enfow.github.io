---
title: '[In-Context RALM] In-Context Retrieval-Augmented Language Models'
date: '2025-03-10'
tags: ['Large-Language-Model', 'RALM']
draft: false
summary: ''
---

# Summary

- Token 단위 Generation 에 검색 결과를 달리하여 추론하는 In-Context RALM을 제안한다.
- 지금까지 생성된 Token 들을 대상으로 Retrieval 하고, 그 결과를 Concatnation 하여 LM 모델에 입력으로 전달한다.
- 컴퓨팅 자원 효율성 및 성능을 고려하여 Retrieval Strid, Retrieval Query Length 등을 하이퍼 파라미터로 추가했다. 

# RALM

RALM(Retrieval-Augmented Language Model) 이란 Language Model이 검색된 관련 문서를 활용하여 추론 결과를 생성하는 방법을 말한다. 기존의 LM 모델은 학습 시에 사용되지 않은 정보들을 활용하지 못하는데, 처음 보는 정보에 대해서도 그럴싸한 답을 내놓는 경우(Hallucination)가 있어 문제로 여겨져 왔다. RALM은 검색을 통해 사용자의 질문과 관련성이 높은 문서들을 찾고, 이를 생성에 적극 활용하도록 함으로써 이러한 문제를 해결하려하는 접근 방법이라 할 수 있다. RALM 을 활용하면 검색기 또는 검색 대상이 되는 데이터셋만 업데이트되면 LM 모델이 학습 시에 보지 못한 데이터에 대해서도 성능을 향상시킬 수 있다.

Retrieval, 즉 검색을 하고 이를 생성에 활용한다고 하면 [RAG(Retrieval Augmented Generation)](https://arxiv.org/pdf/1911.00172)가 가장 먼저 떠오를 수 밖에 없다. RALM은 RAG를 포괄하는 개념이라고 할 수 있는데, RAG는 검색 결과가 되는 문서를 LM 모델 추론 전에 검색을 마치고, 이를 프롬프트에 입력으로 추가하여 전달하는 방식이지만 RALM 은 추론 과정 중간 중간에도 검색을 진행하고, 이를 활용하는 것까지 포함하기 때문이다.

논문에서는 Related Work 에서 RALM 연구에 대해 개괄적으로 다루고 있는데, 크게 두 가지로 분류한다.

## Nearest-neighbor Language Models(kNN-LM)

검색된 가장 가까운 k개의 이웃(nearest neighbors)의 출력을 참조하여, 기존 LM의 확률 분포를 보정하는 방법이다. [Khandelwal et al. (2020)](<https://arxiv.org/pdf/1911.00172>)에서 처음 제안되었다. 분류의 이름에서도 알 수 있듯이 유사도가 높은 문서를 검색하고, 이를 참조하여 기존 LM 의 확률 분포를 보정하는 방법이다.

다음 수식들은 Khandelwal et al. (2020) 논문에서 발췌하였는데, kNN으로 검색된 $$k$$ 개의 검색 결과들에 따른 확률과 기존 LM 모델의 확률을 적절한 비율로 결합하여 다음 단어를 예측하는 데에 활용함을 확인할 수 있다. 

$$
p(y|x) = \lambda \, p_{\text{kNN}}(y|x) + (1 - \lambda) \, p_{\text{LM}}(y|x)
$$

## Retrieval and Read Model

kNN-LM과 Retrieval and Read Model 의 가장 큰 차이점은 Document Selection 과 Document Reading 부분이 명확히 구분되어 있다는 점이다. [RAG (Lewis et al., 2020)](<https://arxiv.org/pdf/2005.11401>), [FiD (Izacard & Grave, 2021)](<https://arxiv.org/abs/2007.01282>), [REALM (Guu et al., 2020)](<https://arxiv.org/abs/2002.08909>), [RETRO (Borgeaud et al., 2022)](<https://arxiv.org/pdf/2112.04426>) 등이 이와 관련된 방법이라고 할 수 있다. 

논문에서 제안하는 In-Context RALM은 이들 연구와 비교해 볼 때 다음 두 가지 지점에서 차이를 가진다고 한다.

- 언어 모델을 추가로 훈련하지 않고, 기존의 사전 훈련된(off-the-shelf) 언어 모델을 문서 읽기에 활용함
- 언어 모델의 성능을 향상시키기 위한 문서 선택 방법에 집중함

# In-Context RALM

In-Context RALM 모델의 기본 수식은 다음과 같다.

$$
p(x_1, \dots, x_n) = \prod_{i=1}^{n} p_{\theta} \left( x_i \middle| \mathcal{R}_{\mathcal{C}}(x_{<i}); x_{<i} \right)
$$

여기서 $$\mathcal{R}_{\mathcal{C}}(x_{<i})$$는 $$(x_{<i})$$를 입력으로 하여 찾은 검색 결과를, $$[a;b]$$ 는 string $$a$$와 $$b$$를 Concat 한 것을 의미하므로, 쉽게 말해 다음 토큰 $$x_i$$를 추론하기 위해 이전에 생성된 토큰들과 이들만을 활용한 검색 결과를 붙인 텍스트를 활용한다는 것이다.

<img
  src="/static/images/paper-review/in-context-ralm-example.png"
  alt="instruct-gpt-3step-diagram"
  className="mx-auto block w-full"
/>

제시된 이미지를 보면 보다 직관적으로 이해할 수 있다. 지금까지 생성된 단어 $$x_{i-1}$$이 "World Cup 2022 was the last with 32 teams, before the increase to" 라고 할 때, 일반적인 생성 모델은 이 단어 시퀀스만 활용하여 다음 단어를 생성하게 되지만, In-Context RALM 은 이들을 Retriever 에 넣어 생성에 필요한 정보 "FIFA World Cup 2026 will expland to 48 teams"를 찾아 두 가지를 Concat 해서 생성하도록 한다. 이를 통해 언어 모델의 추가적인 학습 없이도 실시간 데이터와 같이 새로운 정보들도 잘 다룰 수 있다고 한다.

## Design Choices

기본적인 In Context RALM 모델의 효율성을 높이기 위해 논문에서는 다음 두 가지를 적용해 보았다고 한다.

### Retrieval Stride

첫 번째는 Retrieval Stride, 즉 얼마나 자주 검색을 수행할 것인지에 관한 것이다. 위의 In-Context RALM 기본 수식을 보면 $$x$$를 추론하기 위해 $$\mathcal{R}_{\mathcal{C}}(x_{<i})$$ 를 활용한다. 즉, 매 token generation step 마다 검색을 진행하고, 그 결과를 반영토록 할 수 있다는 것이다.

하지만 이렇게 되면 생성 토큰의 갯수에 비례하여 검색를 수행하게 되고, 이는 곧 과도한 컴퓨팅 리소스를 의미한다. 이를 해소하기 위해매번 하지 않고, 일정 간격 - $$s > 1$$ 토큰 - 을 두고 새로운 문서를 검색하는 방법을 제안하는데, 이 부분이 적용된 수식은 아래와 같다.

$$
p(x_1, \dots, x_n) = \prod_{j=0}^{n_s - 1} \prod_{i=1}^{s} p_{\theta} \left( x_{s \cdot j + i} \middle| \mathcal{R}_{\mathcal{C}}(x_{\leq s \cdot j}); x_{<(s \cdot j + i)} \right)
$$

$$x_{s \cdot j}$$ $$x_{s \cdot (j+1) -1}$$ 토큰을 생성하는 데 있어 $$\mathcal{R}_{\mathcal{C}}(x_{\leq s \cdot j})$$이 고정되어 있음을 알 수 있다. 관련 실험에서 Retrieval Stride $$s$$ 를 길게 잡을수록 성능은 떨어지는 것을 확인하였고, 따라서 연산량과 성능의 트레이드 오프를 관찰할 수 있었다고 한다.

### Retrieval Query Length

$$\mathcal{R}_{\mathcal{C}}(x_{<i})$$ 에서 알 수 있듯이 Retriever 는 지금까지 생성된 전체 Token을 입력으로 받는다. 하지만 새로운 다음 토큰을 추론하는 데에 지금까지 생성된 모든 토큰 정보는 필요하지 않을 수도 있다. 논문에서는 최근에 생성된 토큰들일 수록 직후 토큰을 생성하는 데에 보다 관련성이 높을 것으로 가정하고, 최근 생성 토큰들만 Retriever 에 넣도록 하였다. 여기서 $$l$$이 query length 이다.

$$
q_{j}^{s, \ell} := x_{s \cdot j - \ell + 1}, \dots, x_{s \cdot j}
$$

최종적으로 Retrieval Stride 와 Retrieval Query Length 가 모두 적용된 In-Context RALM 수식은 다음과 같다.

$$
p(x_1, \dots, x_n) = \prod_{j=0}^{n_s - 1} \prod_{i=1}^{s} p_{\theta} \left( x_{s \cdot j + i} \middle| \mathcal{R}_{\mathcal{C}}(q_{j}^{s, \ell}); x_{<(s \cdot j + i)} \right)
$$

Retrieval Stride 와 Retrieval Query Length 두 가지를 함께 적용함에 있어 중요한 점 중 하나는, 두 가지 하이퍼 파라미터의 크기가 같은 경우, 즉 $$s=l$$ 으로 설정하면 성능이 떨어진다는 것이다.

# Reference

- [Khandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer, L., Lewis, M. and University, S. (n.d.). Published as a conference paper at ICLR 2020 GENERALIZATION THROUGH MEMORIZATION: NEAREST NEIGHBOR LANGUAGE MODELS.](<https://arxiv.org/pdf/1911.00172>)
- [Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H., Lewis, M., Yih, W.-T., Rocktäschel, T., Riedel, S., Kiela, D., Facebook and Research, A. (2021). Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.](<https://arxiv.org/pdf/2005.11401>)
- [Izacard, G. and Grave, E. (2020). Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering](<https://arxiv.org/abs/2007.01282.>)
- [Guu, K., Lee, K., Tung, Z., Pasupat, P. and Chang, M.-W. (2020). REALM: Retrieval-Augmented Language Model Pre-Training.](<https://arxiv.org/abs/2002.08909>)
- [Borgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford, E., Millican, K., Van Den Driessche, G., Lespiau, J.-B., Damoc, B., Clark, A., De, D., Casas, L., Guy, A., Menick, J., Ring, R., Hennigan, T., Huang, S., Maggiore, L., Jones, C. and Cassirer, A. (n.d.). Improving language models by retrieving from trillions of tokens](<https://arxiv.org/pdf/2112.04426>)